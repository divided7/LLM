# LoRAè®­ç»ƒ
LoRAçš„æ€æƒ³: 

### Step 1: åˆ›å»ºåŸºç¡€æ¨¡å‹

```python
import torch
import torch.nn as nn

# åŸºç¡€æ¨¡å‹ï¼šä¸€ä¸ªç®€å•çš„å‰é¦ˆç½‘ç»œ
class BaseModel(nn.Module):
    def __init__(self, input_dim=10, hidden_dim=20, output_dim=5):
        super().__init__()
        self.linear = nn.Linear(input_dim, hidden_dim)
        self.output = nn.Linear(hidden_dim, output_dim)

    def forward(self, x):
        x = self.linear(x)
        x = torch.relu(x)
        return self.output(x)
```

---

### Step 2: æ‰‹åŠ¨æ·»åŠ  LoRA æ¨¡å—

LoRA çš„åŸºæœ¬æ€æƒ³æ˜¯ï¼š  
åœ¨åŸå§‹æƒé‡ `W` çš„åŸºç¡€ä¸ŠåŠ ä¸€ä¸ªä½ç§©çŸ©é˜µåˆ†è§£ï¼š`W_delta = A @ B`

```python
class LoRALinear(nn.Module):
    def __init__(self, original_linear: nn.Linear, r=4, alpha=1.0):
        super().__init__()
        self.original_linear = original_linear  # ä¸æ”¹å˜åŸå§‹æƒé‡

        # å†»ç»“åŸå§‹æƒé‡ï¼ˆå¯é€‰ï¼‰
        for param in self.original_linear.parameters():
            param.requires_grad = False

        # æ·»åŠ  LoRA çš„ A, B å±‚ï¼ˆä½ç§©åˆ†è§£ï¼‰
        in_features = original_linear.in_features
        out_features = original_linear.out_features

        self.lora_A = nn.Linear(in_features, r, bias=False)
        self.lora_B = nn.Linear(r, out_features, bias=False)

        # ç¼©æ”¾å› å­
        self.scaling = alpha / r

    def forward(self, x):
        return self.original_linear(x) + self.scaling * self.lora_B(self.lora_A(x))
```

---

### Step 3: æ›¿æ¢åŸå§‹ç½‘ç»œä¸­çš„æŸä¸€å±‚ä¸º LoRA å±‚

```python
# åˆå§‹åŒ–æ¨¡å‹
model = BaseModel()

# æ›¿æ¢ model.linear ä¸ºå¸¦ LoRA çš„ç‰ˆæœ¬
model.linear = LoRALinear(model.linear, r=4, alpha=8)

# æµ‹è¯•
x = torch.randn(2, 10)
output = model(x)
print(output)
```

---

### Training Tip

- ç°åœ¨åªæœ‰ `lora_A` å’Œ `lora_B` æ˜¯å¯è®­ç»ƒçš„ï¼ŒåŸå§‹çš„ `linear.weight` æ˜¯è¢«å†»ç»“çš„ã€‚
- ä½ åªéœ€è¦ä¼˜åŒ–è¿™ä¸¤ä¸ª LoRA å±‚çš„å‚æ•°ï¼Œå°±èƒ½â€œå¾®è°ƒâ€æ¨¡å‹äº†ã€‚

---

### ä¼˜ç‚¹æ€»ç»“

| ä¼˜ç‚¹                 | è¯´æ˜                                         |
|----------------------|----------------------------------------------|
| æ˜¾å­˜å ç”¨å°‘            | åªè®­ç»ƒå°çŸ©é˜µ A å’Œ B                          |
| å¿«é€Ÿè®­ç»ƒ              | å‡å°‘å‚æ•°æ›´æ–°é‡                               |
| å¥½è¿ç§»                | é¢„è®­ç»ƒæ¨¡å‹ä¿æŒä¸å˜ï¼Œé€‚åˆå¤šä»»åŠ¡å¤šè¯­è¨€åœºæ™¯     |
| å¯é€‰åˆå¹¶              | æ¨ç†å‰å¯ä»¥åˆå¹¶ä¸ºæ™®é€š Linear å±‚                |

---

å¦‚æœä½ éœ€è¦æˆ‘å¸®ä½ æ‰©å±•è¿™ä¸ªä¾‹å­ï¼Œæ¯”å¦‚è®­ç»ƒã€ä¿å­˜ã€åˆå¹¶ LoRA æƒé‡ç­‰ï¼Œä¹Ÿå¯ä»¥ç»§ç»­è¯´ ğŸ˜„